{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tpnN_XZwq_vc"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/cholojuanito/deep-learning-cancer-detection/blob/main/unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "# Cancer Detection\n",
    "\n",
    "## Overview\n",
    "Convolutional neural networks are used to solve many computer vision problems. One of the more modern problems researchers and machine learning engineers have tried to solve is detecting cancer in cross-sectioned microscopic slides for various tissue types. These slide images are called \"whole slide images\" (WSI) they are *huge* images, usually 1-2 GBs large and are digitized with fancy scanner machines.\n",
    "\n",
    "Recently the FDA gave the first ever approval for a machine learning based digital pathology product [read more here](https://www.paige.ai/news/news-press-release1/). So the digital pathology industry is still growing and in need of great minds!\n",
    "\n",
    "### What you should already\n",
    "* Know the Python programming language and various packages like numpy and matplotlib\n",
    "* Be familiar with PyTorch\n",
    "* Have a basic understanding of machine and deep learning concepts\n",
    "\n",
    "### You will learn\n",
    "* To build a dense prediction model for image segmentation problems\n",
    "* To understand how to convert research papers into usable networks using PyTorch\n",
    "\n",
    "### Data set\n",
    "The data is given as a set of 1024×1024 PNG images. These images are \"tiles\" or small squares of the original WSI because working with one gigabit file wouldn't fit on most machines and would be extremely slow for training.\n",
    "Each input image (in the ```inputs``` directory) is an RGB image of a section of tissue,\n",
    "and there a file with the same name (in the ```outputs``` directory) \n",
    "that has a dense labeling of whether or not a section of tissue is cancerous\n",
    "(white pixels mean “cancerous”, while black pixels mean “not cancerous”).\n",
    "\n",
    "The data has been pre-split for into test and training splits.\n",
    "Filenames also reflect whether or not the image has any cancer at all \n",
    "(files starting with ```pos_``` have some cancerous pixels, while files \n",
    "starting with ```neg_``` have no cancer anywhere).\n",
    "All of the data is hand-labeled, so the dataset is not very large.\n",
    "This means that overfitting is a real possibility.\n",
    "\n",
    "An example image, and its corresponding ground truth labeling, is shown below.\n",
    "(And is contained in the downloadable dataset below).\n",
    "\n",
    "![](http://liftothers.org/dokuwiki/lib/exe/fetch.php?w=200&tok=a8ac31&media=cs501r_f2016:pos_test_000072_output.png)\n",
    "<img src=\"http://liftothers.org/dokuwiki/lib/exe/fetch.php?media=cs501r_f2016:pos_test_000072.png\" width=\"200\">\n",
    "\n",
    "### Articles & Papers to read beforehand\n",
    "* [U-Net: Convolutional Networks for Biomedical Image Segmentation | Arvix paper](https://arxiv.org/pdf/1505.04597.pdf)\n",
    "* [Up Sampling Images | Toward Data Science](https://towardsdatascience.com/up-sampling-with-transposed-convolution-9ae4f2df52d0)\n",
    "\n",
    "If you don't understand or are new to CNN's then chekcout this article as well\n",
    "* [Intro to Convolutional Neural Networks | Towards Data Science](https://towardsdatascience.com/an-introduction-to-convolutional-neural-networks-eb0b60b58fd7)\n",
    "\n",
    "___\n",
    "\n",
    "## Part 1 - Implement a dense image segmentation network\n",
    "\n",
    "#### Food for thought\n",
    "\n",
    "The simplest network you could implement (with all the desired properties)\n",
    "is just a single convolution layer with two filters and no relu! \n",
    "Why is that? (of course it wouldn't work very well!)\n",
    "\n",
    "#### 1.1 Dissecting the network topology\n",
    "\n",
    "Below is an image from the U-Net paper, it is an illustration of what a U-Net \"looks\" like. You can probably guess why they named it \"U\" net.\n",
    "\n",
    "![(Figure 1)](https://lh3.googleusercontent.com/qnHiB3B2KRxC3NjiSDtY08_DgDGTDsHcO6PP53oNRuct-p2QXCR-gyLkDveO850F2tTAhIOPC5Ha06NP9xq1JPsVAHlQ5UXA5V-9zkUrJHGhP_MNHFoRGnjBz1vn1p8P2rMWhlAb6HQ=w2400)\n",
    "\n",
    "Let's take a look at all the parts of this network illustration.\n",
    "\n",
    "We have, in order from the top-left of the \"U\", down to the bottom, and then up to the top-right of the \"U\":\n",
    "1) 3x3 convolution followed by a Rectified Linear Unit (ReLU)\n",
    "2) 2x2 max pool\n",
    "3) 2x2 up convolution\n",
    "4) 1x1 final convolution\n",
    "5) \"Copy and crop\" for each level\n",
    "\n",
    "Let's dissect each part, its importance and what PyTorch module(s) we will use to implement it.\n",
    "\n",
    "*3x3 convolution and ReLU*\n",
    "\n",
    "These convolutions are \"unpadded\" convolutions, so they will decrease the spatial dimensions of the image slightly, while increasing the number of feature channels. Every one of these convolutions is followed by a ReLU, a non-linear activation function, which provides our network with some non-linearities.\n",
    "\n",
    "PyTorch modules: `Conv2d`, `ReLU`\n",
    "\n",
    "*2x2 max pool*\n",
    "\n",
    "This performs a \"down-sampling\", which doubles the number of feature channels in the image.\n",
    "\n",
    "PyTorch modules: `MaxPool2d`\n",
    "\n",
    "*2x2 up convolution*\n",
    "\n",
    "This is esstentially the opposite of max pool, it will half the number of feature channels in the image.\n",
    "\n",
    "PyTorch modules: `ConvTranspose2d`\n",
    "\n",
    "*1x1 convolution*\n",
    "\n",
    "We use this last convolution to make sure our outputs have the correct dimensionality, basically to make sure we have the correct number of classes at the end. In our case we are just looking for two classes, cancerous and non-cancerous parts of the image.\n",
    "\n",
    "PyTorch modules: `Conv2d`\n",
    "\n",
    "*Copy and crop*\n",
    "\n",
    "This last part is the operation related to the grey arrows that cross the \"U\". We take the *output from the second 3x3 convolution, ReLU combo* at each layer of the \"U\" and crop and concatenate those feature channels to the *output of the up convolution* on the same level from the opposite side of the \"U\". We have to crop because of some pixels that get lost through the convolution process.\n",
    "\n",
    "PyTorch function: `torch.cat` for concatenating across the \"U\"\n",
    "\n",
    "\n",
    "#### 1.2 Creating the U-Net network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install some dependencies if they aren't installed already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%pip3` not found.\n"
     ]
    }
   ],
   "source": [
    "%pip3 install torch\n",
    "%pip3 install torchvision\n",
    "%pip3 install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the imports for all the packages and modules we will use. Run it before you run any other code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wQOefmcZVgTl",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, utils, datasets\n",
    "from tqdm import tqdm\n",
    "from torch.nn.parameter import Parameter\n",
    "import pdb\n",
    "import torchvision\n",
    "import os\n",
    "import gzip\n",
    "import tarfile\n",
    "import gc\n",
    "from IPython.core.ultratb import AutoFormattedTB\n",
    "__ITB__ = AutoFormattedTB(mode = 'Verbose',color_scheme='LightBg', tb_offset = 1)\n",
    "\n",
    "# assert torch.cuda.is_available(), \"You need to request a GPU from Runtime > Change Runtime\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so we know that we need the following PyTorch modules to start making this network:\n",
    "* `Conv2d`\n",
    "* `ReLU`\n",
    "* `MaxPool2d`\n",
    "* `ConvTranspose2d`\n",
    "\n",
    "Let's think about the other info we will need to convert to code about the network. First, we will need to know how many input and output channels the image will have. We will call those `in_channels` and `out_channels`.\n",
    "\n",
    "The network in the paper starts with a convolution that produces 64 feature channels so we will want a variable to hold that information. We also have a set number of \"levels\" or series of convolutional blocks down and up the \"U\". The network in the paper does this 4 times, but we can make it any number we want. Let's call those variables `num_features_start` and `u_depth`.\n",
    "\n",
    "That should be all the hyper-parameters we need to make this network. Below is the network I've made, extending it as a `nn.Module` so it fits seamlessly into PyTorch's computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_features_start=64, u_depth=4):\n",
    "        super(UNet, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = u_depth\n",
    "        size = num_features_start\n",
    "\n",
    "        self.conv = nn.Conv2d\n",
    "        self.activation_func = nn.ReLU\n",
    "\n",
    "        self.down_convs = nn.ModuleList()\n",
    "        self.down_samples = nn.ModuleList()\n",
    "        self.up_samples = nn.ModuleList()\n",
    "        self.up_convs = nn.ModuleList()\n",
    "\n",
    "        previous_size = self.in_channels\n",
    "        current_size = size\n",
    "        # down the U\n",
    "        for i in range(self.depth) :\n",
    "            self.down_convs.append(nn.Sequential(\n",
    "                self.conv(previous_size, current_size, kernel_size=3, stride=1, padding=1),\n",
    "                self.activation_func(),\n",
    "                self.conv(current_size, current_size, kernel_size=3, stride=1, padding=1),\n",
    "                self.activation_func(),\n",
    "            ))\n",
    "            self.down_samples.append(nn.MaxPool2d(kernel_size=2))\n",
    "            previous_size = current_size\n",
    "            current_size *= 2\n",
    "\n",
    "        # bottom convolutions\n",
    "        self.bottom = nn.Sequential(\n",
    "            self.conv(previous_size, current_size, kernel_size=3, stride=1, padding=1),\n",
    "            self.activation_func(),\n",
    "            self.conv(current_size, current_size, kernel_size=3, stride=1, padding=1),\n",
    "            self.activation_func()\n",
    "        )\n",
    "\n",
    "        # up the U\n",
    "        for i in range(self.depth) :\n",
    "            next_size = current_size//2\n",
    "            self.up_samples.append(nn.Sequential(\n",
    "                nn.ConvTranspose2d(current_size, next_size, kernel_size=2, stride=2, padding=0),\n",
    "                self.activation_func(),\n",
    "            ))\n",
    "            self.up_convs.append(nn.Sequential(\n",
    "                self.conv(current_size, next_size, kernel_size=3, stride=1, padding=1),\n",
    "                self.activation_func(),\n",
    "                self.conv(next_size, next_size, kernel_size=3, stride=1, padding=1),\n",
    "                self.activation_func(),\n",
    "            ))\n",
    "            current_size = next_size\n",
    "\n",
    "        # last convolutional layer\n",
    "        self.final = self.conv(current_size, self.out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # go down the U\n",
    "        activations = []\n",
    "        for i in range(self.depth):\n",
    "            x = self.down_convs[i](x)\n",
    "            activations.append(x)\n",
    "            x = self.down_samples[i](x)\n",
    "        \n",
    "        x = self.bottom(x)\n",
    "\n",
    "        # back up the U\n",
    "        for i in range(self.depth):\n",
    "            x = self.up_samples[i](x)\n",
    "            x = torch.cat((activations[-(i+1)], x), 1)\n",
    "            x = self.up_convs[i](x)\n",
    "\n",
    "        return self.final(x)\n",
    "\n",
    "def pad_to_shape(tensor, out_shape):\n",
    "    \"\"\"\n",
    "    Pads this image with zeroes to shp.\n",
    "    Args:\n",
    "        tensor: image tensor to pad\n",
    "        shp: desired output shape\n",
    "    Returns:\n",
    "        Zero-padded tensor of shape shp.\n",
    "    \"\"\"\n",
    "    if len(out_shape) == 4:\n",
    "        pad = (0, out_shape[3] - tensor.shape[3], 0, out_shape[2] - tensor.shape[2])\n",
    "    elif len(out_shape) == 5:\n",
    "        pad = (0, out_shape[4] - tensor.shape[4], 0, out_shape[3] - tensor.shape[3], 0, out_shape[2] - tensor.shape[2])\n",
    "    return F.pad(tensor, pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Part 2 - Dataset\n",
    "\n",
    "### Preparing the dataset\n",
    "Next, we wrap our image dataset into a PyTorch `Dataset` object to make it easier to load the data into memory and create batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Il_53HLSWPTY"
   },
   "outputs": [],
   "source": [
    "class CancerDataset(Dataset):\n",
    "  def __init__(self, root, download=True, size=512, train=True):\n",
    "    if download and not os.path.exists(os.path.join(root, 'cancer_data')):\n",
    "      datasets.utils.download_url('http://liftothers.org/cancer_data.tar.gz', root, 'cancer_data.tar.gz', None)\n",
    "      self.extract_gzip(os.path.join(root, 'cancer_data.tar.gz'))\n",
    "      self.extract_tar(os.path.join(root, 'cancer_data.tar'))\n",
    "    \n",
    "    postfix = 'train' if train else 'test'\n",
    "    root = os.path.join(root, 'cancer_data', 'cancer_data')\n",
    "    self.dataset_folder = torchvision.datasets.ImageFolder(os.path.join(root, 'inputs_' + postfix) ,transform = transforms.Compose([transforms.Resize(size),transforms.ToTensor()]))\n",
    "    self.label_folder = torchvision.datasets.ImageFolder(os.path.join(root, 'outputs_' + postfix) ,transform = transforms.Compose([transforms.Resize(size),transforms.ToTensor()]))\n",
    "\n",
    "  @staticmethod\n",
    "  def extract_gzip(gzip_path, remove_finished=False):\n",
    "    print('Extracting {}'.format(gzip_path))\n",
    "    with open(gzip_path.replace('.gz', ''), 'wb') as out_f, gzip.GzipFile(gzip_path) as zip_f:\n",
    "      out_f.write(zip_f.read())\n",
    "    if remove_finished:\n",
    "      os.unlink(gzip_path)\n",
    "  \n",
    "  @staticmethod\n",
    "  def extract_tar(tar_path):\n",
    "    print('Untarring {}'.format(tar_path))\n",
    "    z = tarfile.TarFile(tar_path)\n",
    "    z.extractall(tar_path.replace('.tar', ''))\n",
    "\n",
    "  def __getitem__(self,index):\n",
    "    img = self.dataset_folder[index]\n",
    "    label = self.label_folder[index]\n",
    "    return img[0],label[0][0]\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.dataset_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Part 3 - Training the model\n",
    "\n",
    "### Trainer class\n",
    "Here we are going to bundle up general training steps into a single class. It doesn't directly rely on PyTorch expect for making sure that the gradient is off for the eval steps. In our case, training involves the following steps:\n",
    "1. For each epoch*:\n",
    "    1. Ensure the gradient is turned on (this is how the network learns through backpropagation)\n",
    "    2. For each batch of images in the **training** dataset:\n",
    "        1. Load the batch into GPU memory\n",
    "        2. Reset the gradient\n",
    "        3. Pass the images through the U-Net\n",
    "        4. Calculate loss/error\n",
    "        5. Track the accuracy\n",
    "        6. Call `backward()` to peform the back propagation\n",
    "        7. Take a \"step\" in the solution space based on the gradient\n",
    "    3. Turn off the gradient\n",
    "    4. For each batch of images in the **validation** dataset:\n",
    "        1. Load the batch into GPU memory\n",
    "        2. Pass the images through the U-Net\n",
    "        3. Calculate loss/error\n",
    "        4. Track the accuracy\n",
    "\n",
    "\\* - **epoch** means one round through both the training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationTrainer:\n",
    "    def __init__(self):\n",
    "        self.network = None\n",
    "        self.optimizer = None\n",
    "        self.loss_func = None\n",
    "        self.train_dataloader = None \n",
    "        self.val_dataloader = None\n",
    "\n",
    "        self.device = 'cuda'\n",
    "        self.epoch_count = 0\n",
    "        self.loop = None\n",
    "\n",
    "    def increment_epoch_count(self):\n",
    "        self.epoch_count += 1\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "\n",
    "        for i in range(1, num_epochs+1):\n",
    "            self.loop = tqdm(total=len(self.train_dataloader), position=0, leave=False)\n",
    "            # Train\n",
    "            print(f'\\nTraining for epoch {i} of {num_epochs}')\n",
    "            train_loss, train_acc = self.train_epoch_(self.train_dataloader, training=True)\n",
    "            self.loop.close()\n",
    "\n",
    "            self.loop = tqdm(total=len(self.val_dataloader), position=0, leave=False)\n",
    "            # Validate\n",
    "            print(f\"\\nValidation for epoch {i} of {num_epochs}\")\n",
    "            val_loss, val_acc = self.train_epoch_(self.val_dataloader, training=False)\n",
    "            self.loop.close()\n",
    "            \n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            train_accuracies.append(train_acc)\n",
    "            val_accuracies.append(val_acc)\n",
    "\n",
    "        return  train_losses, val_losses, train_accuracies, val_accuracies\n",
    "\n",
    "    def train_epoch_(self, data_loader, training=True):\n",
    "        if training:\n",
    "            self.network.train() \n",
    "            torch.set_grad_enabled(True)\n",
    "        else:\n",
    "            self.network.eval()\n",
    "            torch.set_grad_enabled(False)\n",
    "\n",
    "        loss_sum = 0\n",
    "        acc_sum = 0\n",
    "        for i, (imgs, labels) in enumerate(data_loader):\n",
    "            imgs, labels = imgs.to(self.device, non_blocking=True), labels.to(self.device,non_blocking=True) # non_blocking is a speed up (async)\n",
    "            \n",
    "            if training:\n",
    "                self.optimizer.zero_grad() # Set gradient to zero\n",
    "\n",
    "            y_hat = self.network(imgs)\n",
    "            loss = self.loss_func(y_hat, labels.long())\n",
    "            loss_sum += loss.detach().sum().item()\n",
    "\n",
    "            \n",
    "            probs = y_hat.argmax(1)\n",
    "            accuracy = (probs == labels).float().mean()\n",
    "            acc_sum += accuracy.detach().item()\n",
    "\n",
    "            mem_allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "\n",
    "            self.loop.set_description('loss: {:.4f}, accuracy: {:.4f}, mem: {:.2f}'.format(loss.detach().sum().item(), accuracy, mem_allocated))\n",
    "            self.loop.update(1)\n",
    "\n",
    "            if training:\n",
    "                loss.backward() # Compute gradient, for weight with respect to loss\n",
    "                self.optimizer.step() # Take step in the direction of the negative gradient\n",
    "\n",
    "        return loss_sum/i, acc_sum/i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "Now we instantiate anything we need for training. Datasets, dataloaders, the network, loss functions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://liftothers.org/cancer_data.tar.gz to D:\\tmp\\cancer_data.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2750494655/2750494655 [15:32<00:00, 2948261.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting D:\\tmp\\cancer_data.tar.gz\n",
      "Untarring D:\\tmp\\cancer_data.tar\n"
     ]
    }
   ],
   "source": [
    "# Instantiate data sets\n",
    "train_dataset = CancerDataset('D:\\\\tmp', train=True)\n",
    "val_dataset = CancerDataset('D:\\\\tmp', train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NAjagHCdGNAh"
   },
   "outputs": [],
   "source": [
    "# Instantiate data loaders\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          shuffle=True,\n",
    "                          batch_size=4,\n",
    "                          num_workers=4,\n",
    "                          pin_memory=True,)\n",
    "\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        shuffle=True,\n",
    "                        batch_size=4,\n",
    "                        num_workers=4,\n",
    "                        pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the network\n",
    "device = \"cuda\"\n",
    "\n",
    "net = UNet(\n",
    "    3, #RGB\n",
    "    2, #Greyscale\n",
    "    num_features_start=64,\n",
    "    u_depth=4\n",
    ").to(device)\n",
    "\n",
    "# Instantiate loss function and optimizer\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(\n",
    "    net.parameters(),\n",
    "    lr = learning_rate\n",
    ")\n",
    "loss_func = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the trainer\n",
    "trainer = SegmentationTrainer()\n",
    "trainer.network = net\n",
    "trainer.optimizer = optimizer\n",
    "trainer.loss_func = loss_func\n",
    "trainer.train_dataloader = train_loader\n",
    "trainer.val_dataloader = val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Next we will create a loop that goes over the number of epochs we want to train for, in our case we will do 10 so we aren't waiting hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RkieTbwlYWPS"
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "def train_loop(num_epochs):\n",
    "  try:\n",
    "    gc.collect()\n",
    "    print(torch.cuda.memory_allocated() / 1e9)\n",
    "    return trainer.train(num_epochs)\n",
    "    \n",
    "  except:\n",
    "    __ITB__()\n",
    "    \n",
    "train_losses, val_losses, train_acc, val_acc = train_loop(num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WARNING: You may run into an error that says `RuntimeError: CUDA out of memory`\n",
    "\n",
    "In this case, the memory required for your batch is larger than what the GPU is capable of. You can solve this problem by either providing more GPUs/memory or adjusting the image size or the batch size and then restarting the runtime. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CZ062Jv1jIIu"
   },
   "source": [
    "\n",
    "___\n",
    "\n",
    "## Part 4 - Display performance\n",
    "\n",
    "A key part of determining the training performance and potential production performance of any model is to calculate and visualize its accuracy.\n",
    "\n",
    "\n",
    "### Plot performance over time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mTg1jyIsYVZN"
   },
   "outputs": [],
   "source": [
    "# Graphing fucntions\n",
    "def show_loss_graph(title, train_losses, val_losses):\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(val_losses, label=\"Validation Loss\")\n",
    "    plt.title(title)\n",
    "    plt.ylabel(\"Avg. Loss per epoch\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylim(0, max(max(train_losses), max(val_losses)))\n",
    "    plt.xlim(0, len(train_losses))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def show_accuracy_graph(title, train_acc, val_acc):\n",
    "    plt.plot(train_acc, label=\"Train Accuracy\")\n",
    "    plt.plot(val_acc, label=\"Validation Accuracy\")\n",
    "    plt.title(title)\n",
    "    plt.ylabel(\"Avg. Accuracy per epoch (%)\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.xlim(0, len(train_acc))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**NOTE:**\n",
    "\n",
    "Guessing that the pixel is not cancerous every single time will give you an accuracy of ~85%. This is due to the fact that the majority of cases and therefore cells are going to be noncancerous. Cancerous cells are an outlier and we want our network to be able to accurately detect them.\n",
    "The trained network should be able to do better than 85% accuracy if we want to call it a successful model.\n",
    "\n",
    "Below are some example graphs that show plotted accuracy metrics\n",
    "\n",
    "![](http://liftothers.org/dokuwiki/lib/exe/fetch.php?w=400&tok=d23e0b&media=cs501r_f2016:training_accuracy.png)\n",
    "![](http://liftothers.org/dokuwiki/lib/exe/fetch.php?w=400&tok=bb8e3c&media=cs501r_f2016:training_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loss_graph(\"Loss graph\", train_losses, val_losses)\n",
    "\n",
    "show_accuracy_graph(\"Accuracy graph\", train_acc, val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S4s92S2_jQOG"
   },
   "source": [
    "___\n",
    "\n",
    "## Part 5 - Generating predictions\n",
    "\n",
    "This is the real test to see how well your model accomplished its task. We need it to work outside of a training environment and in the real world with novel data points, in our case, new images. This is commonly known as inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's define a function that converts our tensors to images so we can see what they look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tensor_as_img(tensor):\n",
    "    img = tensor.numpy()\n",
    "    plt_img = np.transpose(img, (1, 2, 0))\n",
    "    if (plt_img.shape[2] < 2):\n",
    "        plt.imshow(plt_img.squeeze() * 255, cmap='gray', vmin=0, vmax=255)\n",
    "    else:\n",
    "        plt.imshow(plt_img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "A key aspect of inference is that it needs to be fast AND not update the network further. We want to keep the weights of all our network connections fixed. To ensure things stay put we call the `Module.eval()` method to ensure that the gradient will not be updated by passing images through the network.\n",
    "\n",
    "Here we will just use image #172 from the validation data set since we know its label is marked with some cancerous tissue. This way we can compare our model's prediction output using the base and truth images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XXfG3wClh8an",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "test_img, test_label = val_dataset[172]\n",
    "print('Base Image')\n",
    "show_tensor_as_img(test_img)\n",
    "\n",
    "print('Diagnosis - Truth')\n",
    "show_tensor_as_img(test_label.unsqueeze(0))\n",
    "\n",
    "# We aren't training so we need to put the network into `eval` mode\n",
    "net.eval()\n",
    "\n",
    "img_batch = torch.unsqueeze(test_img, 0)\n",
    "img_batch = img_batch.to(device)\n",
    "prediction = net(img_batch)\n",
    "prediction = prediction.argmax(1).to('cpu')\n",
    "\n",
    "print('Diagnosis - Prediction')\n",
    "show_tensor_as_img(prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Part 6 - Saving the model\n",
    "\n",
    "Alright! We've made a pretty good model. But what go is it if it will disappear the second we end this coding session. We will have to train a new one every time and that's just not an efficient use of resources. So we will learn a few ways we can save our model's state. This will allow us to pick up where we left off if we need to continue training and fine tuning a model or allow us to use the model in a production inference environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "caner_detection_unet.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "122ca43e5be78472441972e8592202447383b9fbd624a1070ba6629154edadb0"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "nteract": {
   "version": "0.28.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
